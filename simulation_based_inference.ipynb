{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implicit Inference vs Explicit Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model is defined implicitly with $\\{(d_i, \\theta_i)\\}_{i=1}^N$\n",
    "\n",
    "> Model is defined with explicit pdfs, e.g., Gaussian, exponential, etc. For example, $d=\\mu(\\theta)+n \\quad n \\sim \\mathcal{N}(0, C)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $d_i$ can come from the following sources:\n",
    "- Found: e.g., astronomical observations;\n",
    "- Generated: (a) Physical simulation (b) Generative model $\\Rightarrow$ often called \"Simulation-based Inference\" (SBI)\n",
    "\n",
    "**Example**<br>\n",
    "Computing the posterior mean when the model is specified by $\\{d_i, \\theta_i\\}_{i=1}^N$ such that $(d, \\theta) \\sim p(d, \\theta)$.\n",
    "\n",
    "We want to $f(d) \\approx \\mathbb{E}(\\theta)_{p(\\theta|d)}$, remerber (from Bayesian Decision Theory):<br>\n",
    "\n",
    "For $L = \\int (f(d)-\\theta)^2 p(d, \\theta) d d\\theta$, $\\frac{\\partial L}{\\partial f}|_{f=\\hat{f}} = 0 \\Rightarrow \\hat{f}(d) = \\mathbb{E}(\\theta)_{p(\\theta|d)}$\n",
    "\n",
    "$L \\approx \\frac{1}{N} \\sum_{i} (f(d_i)-\\theta_i)^2$\n",
    "\n",
    "In practice, we can represent $f(d)$ as a Neural Network, $f_w(d)$, where $w$ is the set of weights and biases of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://raw.githubusercontent.com/bd0525/new2GPR/main/assets/nn_demo001.jpg\" alt=\"nn_demo001\" width=\"400\"/>\n",
    "\n",
    "$h = \\varphi(w_1 d+ b_1)$, $\\hat{\\theta} = \\varphi(w_2 h + b_2)$, $\\varphi$ is a non-linear function (e.g., ReLU, tanh, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN are universal function approximators, it is often straight-forward to find $w_i^*$ and $b_i^*$ such that $f_w(d)$ minimizes a diifferentiable loss function $L[f_w]$.<br>\n",
    "\n",
    "\n",
    "=================================================<br>\n",
    "\n",
    "To get the posterior variance:\n",
    "\n",
    "$L = \\int (g(d) - (\\theta - \\hat{f}_{w|d})^2)^2 p(d, \\theta) d\\theta\\, dd$ to get $\\hat{g}(d) = \\mathbb{E}[(\\theta-\\hat{\\theta})^2]_{p(\\theta|d)}$\n",
    "\n",
    "Posterior median:\n",
    "\n",
    "$L = \\int |\\theta - f(d)| p(\\theta, d) d\\theta\\, dd$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caveats:**<br>\n",
    "> Need enough \"training data\", $\\{d_i, \\theta_i\\}_{i=1}^N$;\n",
    "\n",
    "> $f_w(d)$ must be sufficiently expressive (number of hidden layers, architecture, etc.);\n",
    "\n",
    "> have to be able to find a good minimum of $L[f_w]$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ins>**Finding approximations to the posterior pdf**</ins>\n",
    "\n",
    "Certain NN architectures are designed to represent pdfs, for example:\n",
    "- Normalizing flows (MAF);\n",
    "\n",
    "- Mixture density networks, e.g., $p(x) = \\sum_{i=1} w_i \\mathcal{N}(x; \\mu_i, C_i)$, where $w_i$, $\\mu_i$, $C_i$ are parameters of the neural network, with the help of conditional variables ($p(x|y) =  \\sum_{i=1} w_i(y) \\mathcal{N}(x; \\mu_i(y), C_i(y))$)\n",
    "To solve for $\\hat{p}(\\theta|d)$, maximize the kl divergence:<br>\n",
    "$$\\begin{align*}\n",
    "L &= KL\\left(p(\\theta, d) \\mid\\mid \\hat{p}_{w}(\\theta, d)\\right) - \\lambda \\int \\hat{p} d\\theta\\, dd\\\\\n",
    "&=\\int p(\\theta, d) \\ln p(\\theta, d)/\\hat{p}_{W}(\\theta, d) d\\theta\\, dd - \\lambda \\cdots\\\\\n",
    "&=\\text{const} - \\int p(\\theta, d)\\ln \\hat{p}(\\theta, d)dd\\,d\\theta - \\lambda \\cdots\\\\\n",
    "&\\approx \\text{const} - \\frac{1}{N}\\sum\\ln\\hat{p}(\\theta, d)dd\\,d\\theta\n",
    "\\end{align*}$$\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
